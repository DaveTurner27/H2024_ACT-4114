---
output:
  pdf_document:
    includes:
      before_body: TP-title.tex
      in_header: preamble-latex.tex
---

\centering

\clearpage

\tableofcontents

```{=tex}
\justify  
\clearpage
```
```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE)
```

```{r paquetages, message=FALSE, warning=FALSE, eval=TRUE, include=FALSE, echo = FALSE}
### Liste des paquetages
liste.paquetage <- c("ggplot2", "FactoMineR", "pls", "mice", "car", "MASS",
                     "glmnet", "caret", "FNN", "dplyr", "rpart", "gbm",
                     "rpart.plot","tidyverse", "Matrix", "data.table", "iml")

### On installe les paquetages de la liste qu'on a pas déjà
inst <- liste.paquetage %in% installed.packages()
if(length(liste.paquetage[!inst]) > 0) install.packages(liste.paquetage[!inst])

l <- lapply(liste.paquetage, require, character.only = TRUE)
```

```{r import}
train.dat <- read.csv("train.csv")[, -1]
test <- read.csv("test.csv")[, -1]

for(idx in c("Gender", "Type", "Category",
             "Occupation", "Adind", "Region", "Is_18")){
    train.dat[, idx] <- factor(train.dat[, idx])
    test[, idx] <- factor(test[, idx])
}

# Déviance poisson
dev.pois <- function(preds, clnb){
  2*sum(clnb*log((clnb+ 1E-10)/preds)-(clnb-preds))
}
```

# Introduction

Les accidents automobiles sont fréquents partout à travers le monde. La France n'en fait pas exception. En effet, selon les statistiques de la Sécurité Routière, il y a eu, en 2020, environ 142 000 accidents de la route impliquant au moins un véhicule. Ceci représente approximativement 364 accidents par tranche de 100 000 conducteurs. Ces accidents comprennent une gamme de type, des collisions mineures aux accidents plus graves. Il est donc important pour les assureurs de modéliser efficacement la fréquence des accidents afin de prévoir le nombre de réclamations qu'aura une personne assurée durant la durée de sa police. La sévérité des sinistres est tout aussi importante à modéliser, mais nous allons nous concentrer sur la modélisation de la fréquence.

Plus précisément, dans ce rapport, on modélisera la fréquence des réclamations pour la responsabilité civile (dommages matériels) en assurance automobile. Nos données représentent un portefeuille d'assurés français. Dans le tableau de données original on retrouve :

-   la variable réponse $N$, qui est nommée `Numtppd`;

-   la variable d'exposition $365 t$ (nombre de jours de couverture) sous le nom `Exppdays`. Elle est renommée `Expp` après modification pour représenter la proportion d'année couverte : $t =$ `Exppdays` / 365 jours.

Plusieurs modèles seront étudiés pour modéliser la variable réponse :

-   *GLM* (modèle de référence);

-   *GLM* avec régularisation;

-   Modèle des k plus proches voisins;

-   Arbre de décision;

-   *Bagging;*

-   Forêt aléatoire;

-   *Gradient Boosting.*

En ce qui concerne le jeu de données, il est disponible directement en `R` dans le paquetage `CASdatasets`. Plus précisément, il s'agit du jeu de données `pg15training`. Les données ont été utilisées par l'Institut française des actuaires dans un concours/jeu de tarification. Elles proviennent d'assureurs automobiles privés inconnus. La matrice contient 2 ans d'observations (2009 et 2010) avec 50 000 observations dans chacune de ces deux années.

Pour plus d'information sur les données, il est possible d'aller voir la documentation sur CRAN ou bien simplement de faire la commande suivante en `R` : `help(pg15train.dating)`.

À noter que des modifications ont été faites sur le jeu de données, celles-cis sont documentées dans le premier rapport (Analyse préparatoire).

Dans ce rapport on présentera d'abord un modèle de base (GLM). Ensuite, on parlera des autres modèles étudiés en détaillant la procédure utilisée pour les ajuster. Par la suite, tous les modèles étudiées seront comparer. On interprétera ensuite les deux modèles les plus prometteurs. Enfin, on proposera un choix de modèle finale. Les lecteurs intéressés par la recommandation finale peuvent passer directement à la section conclusion.

# Modèle de base

```{r base_mod}
formula_base <- formula(
    Numtppd ~ Gender + Type + Occupation +I(Age/75)+I((Age/75)^2)+I((Age/75)^3)+
        I((Age/75)^4) + I((Age/75)^5) + I((Age/75)^6) + I((Age/75)^7)+
        Bonus + Poldur + Density + Region + log(Power) +
        offset(I(log(Expp)))
)
mod.base <- glm(
    formula = formula_base,
    family = poisson(link = "log"),
    data = train.dat
)
prev_test.base <- predict(mod.base, newdata=test, type="response")
deviance.base <- dev.pois(prev_test.base, test$Numtppd)
```

Après avoir séparé les données en deux échantillons soit `train` (85 % des données) et `test` (15 % des données), on est prêt à construire le modèle de base. On fait face à un problème de régression avec une variable réponse discrète. La loi de Poisson, la loi Binomiale Négative et la de Poisson gonflée à zéro sont toutes des possibilités pour construire un modèle linéaire généralisé sur des données de comptages.

Dans notre cas, le modèle choisi est le modèle Poisson, car c'est le choix le plus simple pour un modèle de base. Le modèle est donc un GLM Poisson avec fonction de lien logarithmique.

Avec notre premier rapport, on a vu qu'il pourrait être intéressant d’utiliser la variable `age` jusqu'au degré 7 (âge, âge$^2, \dots$, âge$^7$). Cela veut dire que la relation entre la variable `age` et notre variable réponse est complexe. Pour mieux l'expliquer, on vient donc utiliser un polynôme de degré 7. À première vue, cette technique semble effectivement efficace, puisqu'après la sélection de variables, on conserve les 7 variables reliées à l'age. Pour sélectionner nos variables, on a fait une analyse de déviance au seuil de confiance 99 % en utilisant la méthode algorithmique *Backward*. C'est donc un modèle simple, mais qui demeure une référence fiable pour la suite.

# Ajustement des modèles

## Régression régularisée Lasso

```{r Lasso, message=FALSE, warning=TRUE}
# Mod sans reg
mod.complet <- glm(Numtppd ~.+I(Age^2)+I(Age^3)+
                   I(Age^4) + I(Age^5) + I(Age^6) +
                   I(Age^7)-Expp-Power-Is_18+I(log(Power)),
                   family = poisson, data = train.dat)

# Data
x.train <- model.matrix(mod.complet, data = train.dat)[, -1]
y.train <- train.dat$Numtppd
x.test <- model.matrix(mod.complet, data = test)[, -1]

# # lambda
# set.seed(111)
# cv.out <- cv.glmnet(
#     x.train, y.train, offset = I(log(train.dat$Expp)),
#     alpha = 1, nfolds = 10, family = "poisson",
# )
# meilleur.lam.lasso <- cv.out$lambda.min
# mod.lasso <- glmnet(x.train, y.train, family = "poisson",
#                     alpha = 1, lambda = meilleur.lam.lasso, offset = log(train.dat$Expp))
```

```{r rds_lasso}
# saveRDS(list(
#     cv.out=cv.out,
#     meilleur.lam.lasso=meilleur.lam.lasso,
#     mod.lasso=mod.lasso), "lasso")
lasso <- readRDS("lasso")
cv.out <- lasso$cv.out
meilleur.lam.lasso <- lasso$meilleur.lam.lasso
mod.lasso <- lasso$mod.lasso

# Prév
prev_test.lasso <- exp(predict(mod.lasso, newx = x.test, newoffset = I(log(test$Expp))))
deviance.lasso <- dev.pois(prev_test.lasso, test$Numtppd)
```

Ici, on construit un modèle de régression régularisée *Lasso*. La distribution utilisée est une fois de plus la loi de Poisson. Le lien utilisé est le lien logarithmique. On rappelle qu'on utilise encore une régression polynomiale de degré 7 sur l'âge.

L'avantage de la régression régularisée *Lasso* est que la sélection des variables explicatives est fait pour nous. La fonction *Score* de la régression est :

$$ S^{Lasso}(\beta) = -l(\beta)/n + \lambda  \sum_{j = 1}^p | \beta_j|. $$

Le vecteur $\beta$ est le vecteur de paramètres qui sera estimé par maximum de vraisemblance. La fonction $l(\beta)$ est la fonction de log-vraisemblance. La variable $n$ est le nombre d'observations dans l'échantillon d’entraînement et la variable $p$ est le nombre de variables explicatives.

Pour ce qui est de l'hyperparamètre $\lambda$, il est choisi à l'aide d'une validation croisée avec 10 plis. Cet hyperparamètre est choisi de façon à minimiser la moyenne de la déviance par observation trouvée par validation croisée. Sa valeur est `r meilleur.lam.lasso`. La figure 1 ci-dessous montre la valeur de la déviance moyenne en fonction de l'hyperparamètre $\lambda$. La valeur qui minimise la déviance est sélectionnée. On voit avec la figure 1 que la régression ne semble pas très utile, puisque la courbe semble croissante et monotone.

```{r fig_lam_lasso, fig.cap="Optimisation de de l'hyperparamètre lambda (Lasso)", fig.align = "center", fig.height=4}
ggplot(mapping = aes(x=cv.out$lambda, y=cv.out$cvm)) +
    geom_point() + geom_line() +
    geom_point(aes(x=meilleur.lam.lasso,
               y=min(cv.out$cvm)), size=5, col="orange", alpha=0.6)+
    theme_classic() + scale_x_continuous(trans = "log10")+
    labs(title="Déviance moyenne en fonction de lambda (Lasso)",
         x="lambda", y="Déviance moyenne")
```

## k plus proches voisins

```{r knn_optimise_k, include=FALSE}

iter <- 30
k <- seq(75, 105, 5)
    keep.cols <- c("Numtppd", "Expp", "Age",
                   "Bonus", "Poldur", "Value",
                   "Density", "Power")
# resultats.knn <- matrix(nrow = iter, ncol = length(k))
# 
# set.seed(55)
# for(i in 1:iter){
# 
#     # Création du jeu de validation
#     p <- 0.3
#     idx_not0 <- which(train.dat$Numtppd > 0)
#     idx_0 <- which(train.dat$Numtppd == 0)
#     ind.val <- c(
#         sample(idx_0, round(p*length(idx_0))),
#         sample(idx_not0, round(p*length(idx_not0))))
#     dat.valid <- train.dat[ind.val, keep.cols]
#     dat.non.valid <- train.dat[-ind.val, keep.cols]
# 
#     # initier la boucle
#     idx <- 0
# 
#     for(k_neib in k){
# 
#         idx <- idx + 1
# 
#         # fit knn
#         fit <- knn.reg(
#             train = dat.non.valid[, -c(1, 2)],
#             y = dat.non.valid$Numtppd / dat.non.valid$Expp,
#             test = dat.valid[, -c(1, 2)],
#             k = k_neib)
# 
#         # Prev
#         prevs <- fit$pred * dat.valid$Expp + 1E-4
#         resultats.knn[i, idx] <- dev.pois(preds = prevs, clnb = dat.valid$Numtppd)
#     }
# }
```

```{r RDS_knn}
# saveRDS(resultats.knn, "knn")
resultats.knn <- readRDS("knn")

# Faire les prév test
train.knn <- train.dat[, keep.cols]
test.knn <- test[, keep.cols]

fit_knn_test <- knn.reg(
    train = train.knn[, -c(1, 2)],
    y = train.knn$Numtppd / train.knn$Expp,
    test = test.knn[, -c(1, 2)],
    k = k[which.min(colMeans(resultats.knn))])

prev_test.knn <- fit_knn_test$pred * test$Expp + 1E-4
deviance.knn <- dev.pois(prev_test.knn, test.knn$Numtppd)
```

Ici, on construit le modèle des k plus proches voisins. Nous devons nous limiter aux variables numériques pour construire ce modèle, on ne s'attend donc pas à avoir le meilleur modèle.

Pour gérer l’exposition au risque, on utilise le ratio $N/t$ comme variable réponse. Le modèle fait une prévision pour une exposition de 1. Il suffit de multiplier par l'exposition réelle afin de trouver la prévision finale.

Le modèle comporte un seul hyperparamètre, soit le $k$ qui représente le nombre de voisins utilisés pour calculer les prévisions. Pour ce faire, on utilise la procédure suivante :

1.  Créer un échantillon de validation de manière aléatoire avec 30 % de l'échantillon d'entrainement.

2.  Faire les prévisions sur chaque observation de l'échantillon de validation, et ce, avec toutes les valeurs de $k$ testées. Note : on utilise un léger biais de 1E-4 pour éviter les prévisions de 0.

3.  Calculer la déviance Poisson de l'échantillon de validation pour toutes les valeurs de $k$.

4.  Répéter 30 fois les étapes 1 à 3.

5.  Utiliser la valeur de $k$ qui minimise la déviance moyenne (voir graphique 2).

À l'aide du graphique 2, on voit très clairement que la valeur optimale pour le $k$ est 95.

```{r fig_knn, fig.cap="Optimisation de de l'hyperparamètre k (knn)", fig.align = "center", fig.height=4}
ggplot(mapping = aes(x=k, y=colMeans(resultats.knn))) + geom_point() + 
     geom_point(aes(x=k[which.min(colMeans(resultats.knn))]),
               y=min(colMeans(resultats.knn)), size=5, col="orange", alpha=0.6)+
    geom_line() + theme_classic() + 
    labs(y="Moyenne des déviances\ndes échantillons  de validations", x="Nombre de voisins (k)",
         title = "Déviance moyenne en fonction de k (knn)")
```

## Arbre de décision

```{r arbre, message=FALSE, warning=FALSE}
set.seed(2019)
tree.control <- rpart.control(
    cp = 0,
    minbucket = 85L,
    xval = 10L)

arbre <- rpart(
    cbind(Expp, Numtppd)~.,
    method = "poisson",
    data = train.dat,
    control = tree.control, parms = list(shrink=1))

# Meilleur cp
# err.min <- arbre$cptable[which.min(arbre$cptable[,4]),]
# err.max <- err.min[4] + err.min[5]
# cp.pois <- max(arbre$cptable[arbre$cptable[,4] <= err.max,1])
cp.pois <- arbre$cptable[which.min(arbre$cptable[,4]), 1]

mod.arbre <- rpart::prune(arbre, cp = cp.pois)
prev_test.arbre <- predict(mod.arbre, newdata=test)
deviance.arbre <- dev.pois(prev_test.arbre, test$Numtppd)
```

On construit maintenant un arbre de régression Poisson. On choisit la déviance Poisson comme fonction de perte. Ce choix est assez intuitif étant donné que nous sommes en présence de données de comptage.

Avant de faire un arbre, il faut choisir le coefficient de variation de la distribution à priori $(\gamma)$. On décide de garder la valeur par défaut que `R` nous suggère pour cet hyperparamètre, puisque de toute façon, ce dernier ne devrait pas influencer la performance du modèle de manière significative.

On utilise un nombre d'observations par feuille minimal de 85. Cette valeur choisie au jugement permet d'avoir un minimum de 0.1 % des observations par feuille. Ce hyperparamètre n'a pas besoin d'être précis, puisque le cp va s'occuper de l'élagage.

On procède par une validation croisée à 10 plis afin de trouver la valeur idéale du cp. On choisit le cp qui donne la plus petite déviance moyenne de la validation croisée. La valeur idéale trouvée est `r cp.pois`. Avec cette valeur, on peut obtenir l'arbre final. Le graphique 3 permet d'illustrer le processus d'optimisation de l'hyperparamètre.

```{r fig_cp_arbre, fig.cap="Optimisation de de l'hyperparamètre cp (arbre)", fig.align = "center", fig.height=4}
ggplot(mapping = aes(x=CP, y=xerror)) + geom_point(data=arbre$cptable) +
    geom_line(data=arbre$cptable) + theme_classic() +
  scale_x_continuous(trans = "log10") + 
    geom_point(aes(x=cp.pois,
                   y=arbre$cptable[which.min(arbre$cptable[, "xerror"]),
                                   "xerror"]), alpha=0.6, 
               size=5, col="orange") +
    ylim(0.8, 0.92)+
    labs(title = "Erreur relative de la validation croisée en fonction du cp",
         x="cp", y="Erreur rel. val. croisée")
```

## Bagging

```{r optimisation_minbucket_bag, include=FALSE}

# set.seed(154)
minb <- seq(0, 225, 25)

# Optimiser minbucket
resultats.bag <- numeric(length(minb))

# idx <- 0
# for(i in minb){
# 
#     idx <- idx + 1
# 
#     # Construire la foret
#     foret.node <- distRforest::rforest(
#         cbind(Expp, Numtppd)~.,
#         method="poisson",
#         data=train.dat,
#         control=rpart.control(
#             minbucket = i,
#             cp=0,
#             xval = 1
#         ),
#         ncand=13,
#         ntrees = 200,
#         subsample = 1,
#         track_oob = TRUE)
# 
#     # Ajout de la deviance
#     resultats.bag[idx] <- tail(foret.node$oob, 1)
# }
```

```{r optimisation_tree_bag, include=FALSE}
# # Optimiser nombre d'arbres
# mod.bag <- distRforest::rforest(
#         cbind(Expp, Numtppd)~.,
#         method="poisson",
#         data=train.dat,
#         control=rpart.control(
#             minbucket = minb[which.min(resultats.bag)],
#             cp=0,
#             xval = 1
#         ),
#         ncand=13,
#         ntrees = 200,
#         subsample = 1,
#         track_oob = TRUE)
```

```{r RDS_bag}

# # Save
# saveRDS(
#     list(
#         resultats.bag=resultats.bag,
#         mod.bag=mod.bag
#     ), "bagging"
# )

# Read
bagging <- readRDS("bagging")
resultats.bag <- bagging$resultats.bag
mod.bag <- bagging$mod.bag

# Prev bag
prev_test.bag <- distRforest::predict.rforest(mod.bag, newdata = test)
deviance.bag <- dev.pois(preds = prev_test.bag, clnb = test$Numtppd)
```

On construit ici un modèle de *bagging*. La fonction de perte utilisée est encore la déviance poisson.

Deux hyperparamètres sont à optimiser soit le nombre d'observations minimal par feuille et le nombre d'arbres utilisées. Pour l'optimisation du nombre minimal d'observations par feuille (*minbucket*), on a entrainé différents modèles identiques, mais avec une valeur de *minbucket* différente allant de 0 à 225 avec des bonds de 25. Pour tous ces modèles, on utilise 200 arbres. Le *minbucket* choisi est celui qui optimise la déviance moyenne des observations hors échantillon (OOB). Le graphique 4 illustre le processus d'optimisation. La valeur choisie est de 100 tel qu'illustrée par le graphique. Le choix de prendre 200 arbres a été vérifié graphiquement avec l'erreur OOB en fonction du nombre d'arbres.

```{r fig_minb_bag, fig.cap="Optimisation de l'hyperparamètre minbucket (bagging)", fig.align = "center", fig.height=4}
ggplot(mapping = aes(x=minb[-1], y=resultats.bag[-1])) +
    geom_point() + geom_line() +
    geom_point(aes(x=minb[which.min(resultats.bag)]),
               y=min(resultats.bag), size=5, col="orange", alpha=0.6) +
    theme_classic() +
        labs(title = "Déviance moyenne OOB en fonction de 'minbucket'",
             x="minbucket", y="Déviance moyenne OOB")
```

```{r fig_tree_bag, eval=FALSE}
ggplot(mapping = aes(x=seq(10, 200, 10), y=mod.bag$oob_error[seq(10, 200, 10)])) + geom_line() + geom_point() + theme_classic() +
    labs(title = "Déviance moyenne OOB en fonction du nombre d'arbres", x="Nombre d'arbres", y="Déviance moyenne OOB")
```

## Forêt aléatoire

```{r optimisation_foret, include=FALSE}
# set.seed(7815)

# Valeurs à tester
ncand <- 3:11
minb.foret <- c(20, 40, 60, 80)

# # initialisation boule
# resultats.foret <- data.frame(ncand=ncand)
# idx_j  <- 0
# 
# for(j in minb.foret){
# 
#     idx_j  <- idx_j + 1
# 
#     # Optimiser ncand
#     prev.ncand <- numeric(length(ncand))
# 
#     idx_i <- 0
#     for(i in ncand){
# 
#         idx_i <- idx_i + 1
# 
#         # Construire la foret
#         foret <- distRforest::rforest(
#             cbind(Expp, Numtppd)~.,
#             method="poisson",
#             data=train.dat,
#             control=rpart.control(
#                 minbucket = j,
#                 cp=0,
#                 xval = 1
#             ),
#             ncand=i,
#             ntrees = 125,
#             subsample = 0.5,
#             track_oob = TRUE)
# 
#         # Ajout de l'erreur
#         prev.ncand[idx_i] <- tail(foret$oob_error, 1)
#     }
#     resultats.foret[, as.character(j)] <- prev.ncand
# }
# 
# # Reshape the data
# resultats.foret <- resultats.foret %>%
#   pivot_longer(cols = 2:ncol(resultats.foret),
#                names_to = "minb",
#                values_to = "Erreur")
```

```{r foret_final, include=FALSE}
# set.seed(4230)
# mod.foret <- distRforest::rforest(
#         cbind(Expp, Numtppd)~.,
#         method="poisson",
#         data=train.dat,
#         control=rpart.control(
#             minbucket = as.numeric(
#                 resultats.foret[which.min(resultats.foret$Erreur), ]$minb),
#             cp=0,
#             xval = 0
#         ),
#         ncand=as.numeric(
#             resultats.foret[which.min(resultats.foret$Erreur), ]$ncand),
#         ntrees = 150,
#         subsample = 0.5,
#         track_oob = T)
```

```{r RDS_foret}

# # Save
# saveRDS(
#     list(
#         mod.foret=mod.foret,
#         resultats.foret=resultats.foret
#     ), "foret"
# )

# Read
foret <- readRDS("foret")
resultats.foret <- foret$resultats.foret
mod.foret <- foret$mod.foret

# Prev foret
prev_test.foret <- distRforest::predict.rforest(mod.foret, newdata = test)
deviance.foret <- dev.pois(preds = prev_test.foret, clnb = test$Numtppd)
```

On construit maintenant un modèle de forêt aléatoire. La fonction de perte utilisée est la déviance Poisson.

Plusieurs hyperparamètres sont à déterminer :

-   proportion du nombre d'observations des échantillons *bootstrap*;

-   nombre minimal d'observations par feuille (*minbucket*);

-   nombre de variables candidates (m);

-   nombre d'arbres dans la forêt.

On utilise une proportion de 50 % du nombre d'observations pour faire les échantillons *bootstrap.* Ce choix est motivé par un souci de puissance de calculs. De plus, la valeur choisit ne devrait pas modifier de manière très significative la performance du modèle.

Le nombre d'arbre est initialement déterminé à 125 puisque ce nombre semblait suffisant avec un graphique OOB sur un modèle initial avec hyperparamètres non optimisés. Ce nombre permet aussi d'avoir des temps de calculs plus raisonnable pour l'optimisation des hyperparamètres m et *minbucket*. La validité de ce nombre sera revérifiée à la fin du processus d'optimisation.

Pour optimiser m et *minbucket*, on construit un modèle pour toutes les combinaisons possibles des valeurs d'hyperparamètres testées. Pour chaque modèle, on calcule la déviance moyenne OOB. On teste les valeurs d'hyperparamètres suivantes :

-   $m \in [3, 4, \dots, 11]$;

-   $minbucket \in [20, 40, 60, 80]$.

Le graphique 5 permet de visualiser le processus. On voit sur le graphique que la combinaison *minbucket* = 40 et m = 8 semble être la combinaison optimale.

On finit la modélisation en ré-entraînant le modèle avec les hyperparamètres optimaux et on choisit un nombre d'arbres de 150 pour être sûr d'en avoir suffisamment.

```{r fig_optim_foret, fig.cap="Optimisation des hyperparamètres m et minbucket (RF)", fig.align = "center", fig.height=4}
ggplot(resultats.foret, mapping = aes(x=ncand, y=Erreur, col=minb)) +
    geom_point(size=2, alpha=0.6) + geom_line(linewidth=1) + theme_classic() + theme(legend.position = "bottom") +
    scale_x_continuous(breaks = seq(min(resultats.foret$ncand), max(resultats.foret$ncand), by = 1))+
    labs(title = "Déviance OOB en fonction des hyperparamètres m' et 'MinBucket'", x="Nombre de variables candidates (m)",
         y="Déviance moyenne OOB", col="Minimum d'obs. par feuille :")
```

## Gradient Boosting

Finalement, on fait le modèle de *boosting* de gradient stochastique. La fonction de perte utilisée est la déviance Poisson.

Plusieurs hyperparamètres sont à déterminer :

-   proportion du nombre d'observations des échantillons *bootstrap* (*bag.fraction*);

-   profondeur maximale des arbres (*max.depth*);

-   nombre minimal d'observations par feuille (*n.minobsinnode*);

-   taux d'apprentissage (*shrinkage*);

-   nombre d'itérations (*iter*).

Les hyperparamètres *shrinkage* et *n.minobsinnode* ont été posés respectivement à 0.01 et 100. La valeur de *shrinkage* a été choisie de manière à prendre la plus petite valeur qui permet d'avoir un temps de calculs acceptable. La valeur de *n.minobsinnode* est choisie arbitrairement pour avoir une proportion d'observations minimale dans chaque feuille. Le choix de cette dernière n'est pas critique, car *max.depth* limite le surajustement.

Pour optimiser *bag.fraction, iter* et *max.depth*, on étudie les valeurs suivantes :

-   $\text{max.depth} \in [3, 4, 5, 7, 9]$;

-   $\text{bag.fraction} \in [0.5, 0.65, 0.8]$.

Pour choisir la meilleure combinaison, on utilise la procédure suivante :

1.  Créer un échantillon de validation avec 15 000 observations.

2.  Entraîner un modèle avec une validation croisée avec 5 plis et une combinaison des hyperparamètres *max.depth* et *bag.fraction.*

3.  Utiliser la valeur d'*iter* qui minimise la déviance de la validation croisée.

4.  Calculer la déviance sur l'échantillon de validation en utilisant les hyperparamètres choisis.

5.  Répéter les étapes 2, 3, et 4 avec une combinaison d'hyperparamètres différente et ce jusqu'à ce que toutes les combinaisons aillent été testées.

On peut étudier les résultats dans le graphique 6 où il est possible de voir que la meilleure combinaison de *max.depth* et *bag.fraction* est de 3 et 0.65. Le nombre d'itérations optimal pour cette combinaison est 6873.

```{r}
set.seed(201)
valid_idx <- sample(85000, 15000)
nbTrees <- c(
    2000, 2000,
    1750, 1750,
    1500, 1500,
    1000, 1000,
    750, 750)

# Valeurs à tester
max.depth <- c(1, 2, 3, 4, 5)
bag.fraction <- c(0.5, 0.75)

# # initier la boucle
# resultats.boost <- data.frame()
# 
# idx <- 0
# for(j in max.depth){
#     
#     for(i in bag.fraction){
#         
#         idx <- idx + 1
#         
#         gbm_mtpl <- gbm(
#             Numtppd ~.-Expp, 
#             data = train.dat[-valid_idx,], 
#             distribution = "poisson",
#             weights = Expp,
#             n.trees = nbTrees[idx], 
#             shrinkage = 0.05, 
#             interaction.depth = j, 
#             bag.fraction = i, 
#             n.minobsinnode = 100, 
#             verbose = TRUE, 
#             cv.folds = 5) 
#           
#         # Arbres
#         best.iter.cv <- gbm.perf(gbm_mtpl, method = "cv")
#         
#         # Prev
#         prev <- predict(gbm_mtpl, newdata = train.dat[valid_idx,],
#                         n.trees = best.iter.cv,
#                       type = "response")*train.dat[valid_idx, "Expp"]
#         dev <- dev.pois(prev, train.dat[valid_idx, 11])
#         
#         # Results
#         resultats.boost[idx, "dev"] <- dev
#         resultats.boost[idx, "depth"] <- j
#         resultats.boost[idx, "bag.frac"] <- i
#         resultats.boost[idx, "iter"] <- best.iter.cv
#     }
# }
```

```{r}
# idx <- which.min(resultats.boost[, "dev"])
# mod.boost <- gbm(
#             Numtppd ~.-Expp, 
#             data = train.dat[-valid_idx,], 
#             distribution = "poisson",
#             weights = Expp,
#             n.trees = 2250, 
#             shrinkage = 0.05, 
#             interaction.depth = resultats.boost[idx, "depth"], 
#             bag.fraction = resultats.boost[idx, "bag.frac"], 
#             n.minobsinnode = 100, 
#             verbose = TRUE, 
#             cv.folds = 5)
# best.iter.cv <- gbm.perf(mod.boost, method = "cv")

# set.seed(125)
# 
# controles <- trainControl(method="none", number=1)
# 
# gbmGrille <-  expand.grid(n.trees = 1800,
#                          interaction.depth = 2,
#                          shrinkage = 0.05,
#                          n.minobsinnode = 100)

# mod.gbm <- caret::train(Numtppd ~.-Expp,
#                 data = train.dat[-valid_idx,],
#                 method = "gbm",
#                 bag.fraction=0.75,
#                 weights=Expp, 
#                 distribution = "poisson",
#                 trControl = controles,
#                 tuneGrid = gbmGrille,
#                 verbose = TRUE)
# mod.boost <- mod.gbm
```

```{r}
# saveRDS(
#     list(
#         resultats.boost=resultats.boost,
#         mod.boost=mod.boost,
#         best.iter.cv=best.iter.cv
#     ), "boost"
# )

boost <- readRDS("boost")
mod.boost <- boost$mod.boost
resultats.boost <- boost$resultats.boost
best.iter.cv <- boost$best.iter.cv

prev_test.boost <- predict(mod.boost, newdata = test,type = "response",
                           n.trees=best.iter.cv) * test$Expp
deviance.boost <- dev.pois(prev_test.boost, test$Numtppd)
```

```{r fig_optim_boost, fig.cap="Optimisation des hyperparamètres max.depth et bag.fractions (boosting)", fig.align = "center", fig.height=4}
ggplot(resultats.boost, mapping = aes(x=depth, y=dev, col=factor(bag.frac))) +
    geom_point(size=2, alpha=0.6) + geom_line(linewidth=1) +
    theme_classic() + theme(legend.position = "bottom") +
    labs(title = "Déviance de l'échantillon valid en fonction de max.depth et bag.fraction", x="max.depth",
         y="Déviance valid", col="bag.fraction : ")
```

# Comparaison des modèles

Dans cette section, on compare les modèles afin de choisir les deux plus performant pour la suite. La métrique de choix pour notre situation est la déviance Poisson. On présente donc dans la table 1 la déviance Poisson des différents modèles sur les données `test` qui n'ont pas servi à l'entraînement. Ce que la table nous indique est que les meilleurs modèles sont : le modèle de base, le modèle *Lasso* et le modèle de *boosting*. Les autres modèles sembles nettement moins performants que ces trois deniers si on se fie à la métrique de déviance.

```{r table deviance}
dev.df <- data.frame(
    Modèle = c(
        "Base (GLM)", "Lasso", "Knn",
        "Arbre", "Bagging",
        "RF", "Boost"
    ),
    Déviance=round(c(
        deviance.base, deviance.lasso, deviance.knn,
        deviance.arbre, deviance.bag,
        deviance.foret, deviance.boost
    )),
    LR=round(sapply(list(
        prev_test.base, prev_test.lasso, prev_test.knn,
        prev_test.arbre, prev_test.bag,
        prev_test.foret, prev_test.boost), sum
    ) / sum(test$Numtppd), 2)
)
dev.df <- dev.df[order(dev.df$Déviance), ]
knitr::kable(dev.df, format = "markdown", row.names = FALSE, align = "c",
             caption = "Métriques de performance sur l'échantillon Test")
```

```{r}
# df <- cbind(
#     prev_test.base, unlist(prev_test.lasso), prev_test.boost,
#     prev_test.foret, prev_test.bag, prev_test.arbre, prev_test.knn
# )
# colnames(df)[2] <- "prev_test.lasso"
# round(cor(df), 2)

# Moyen alternatif d'ajouter des variables
test_gini <- data.frame(test$Numtppd, prev_test.base,
                        prev_test.lasso, prev_test.boost,
                        prev_test.foret, prev_test.knn,
                        prev_test.arbre, prev_test.bag)

# On peux maintenant créer un dataset léger qui contient ce qu'on veut
colnames(test_gini) <- c("Nb réclamtion", "GLM", "Lasso", "Boosting", "Random F",
                          "Knn", "Arbre", "Bagging")

gini <- cplm::gini(loss = "Nb réclamtion",
score = colnames(test_gini[-1]),
data = test_gini)
gini <- data.frame(
  GLM = c(0.0000, 3.6734, 1.1772, 14.6824, 39.5379, 41.1209, 15.9896),
  Lasso = c(2.2519, 0.0000, 1.9021, 14.8835, 39.4904, 41.0430, 16.1354),
  Boosting = c(13.4837, 13.6122, 0.0000, 19.2868, 41.4998, 42.9069, 20.1286),
  `Random F` = c(8.1099, 8.1246, 1.9569, 0.0000, 37.7505, 38.9984, 6.4666),
  Knn = c(1.7785, 1.1355, 1.1607, 2.1880, 0.0000, 23.8459, 4.5771),
  Arbre = c(3.1232, 2.6561, 2.2351, -1.3346, 18.4602, 0.0000, 1.0112),
  Bagging = c(7.9300, 8.0732, 1.9834, -0.2761, 37.5180, 38.6624, 0.0000)
)
gini <- round(gini, 2)
diag(gini) <- "-"
rownames(gini) <- c("GLM", "Lasso", "Boosting", "Random F",
                    "Knn", "Arbre", "Bagging")
knitr::kable(gini, format = "markdown", row.names = TRUE, align = "c",
             caption = "Coef. de Gini - (Modèle de référence sur la gauche)")
```

```{r}

# double lift curve graph
double_lift <- function(actual, ref, pred, mod.names, k=4, w){
    
    df <- data.frame()
    idx <- 1
    n <- length(ref)
    
    for(p in pred){
        
        idx <- idx + 1
        p <- unlist(p)
        r <- unlist(ref)
        
        # Préparation
        idx.order <- order(p/r)
        scale <- sum(w) / k * 0.7 
    
        # Sort
        r <- r[idx.order]
        p <- p[idx.order]
        a <- actual[idx.order]
        data <- data.frame(
            FIGURE=c(r, p),
            Model=as.factor(c(rep(mod.names[1], n), rep(mod.names[idx], n))),
            w=rep(w, 2)
        )
    
        # Cut
        data$GROUPE <- rep(cut(1:n, k, labels = F), 2)
    
        # LR
        sum_actual <- sapply(split(a, cut(1:n, k, labels = F)), sum)
        data <- data %>%
            group_by(Model, GROUPE) %>%
            summarise(LR = sum(FIGURE), EU=sum(w))
        data$LR <- sum_actual / data$LR
        data$WHICH_MODEL <- rep(mod.names[idx], nrow(data))
        
        # Bind
        df <- rbind(df, data)
    }
    df$WHICH_MODEL <- factor(df$WHICH_MODEL) 

    # Graphique
    p <- ggplot(df) + facet_wrap(~WHICH_MODEL, ncol = 3)+
        geom_point(aes(x=GROUPE, y=LR, col=Model), size=2.5) +
        geom_line(aes(x=GROUPE, y=LR, col=Model), linewidth=1) +
        geom_hline(yintercept = 1, linewidth=0.4, col="black", alpha=0.65)+
        geom_bar(stat="identity", aes(x = GROUPE,  y=EU/scale),
                 fill="grey", width=0.6, alpha=0.225, position = "dodge") +
        scale_y_continuous(
            name="Ratio de perte",
            sec.axis = sec_axis(~.*scale, name = "Exposition", )) +
        labs(title = "Courbes 'double lift' avec GLM comme référence",
             x="Groupe", col="Modèles :") + 
        theme_bw() + coord_cartesian(ylim=c(0.4, 1.6))+
        theme(legend.position = "bottom")
    p
}
```

```{r, fig.cap="Comparaison des modèles", fig.align = "center", fig.height=6, message=FALSE}
library(gridExtra)
double_lift(actual = test$Numtppd, ref = prev_test.base,
            pred = list(prev_test.lasso, prev_test.boost, prev_test.foret,
                        prev_test.knn, prev_test.arbre, prev_test.bag),
            mod.names = c("GLM", "Lasso", "Boosting", "Random F",
                          "Knn", "Arbre", "Bagging"),
            k=5, w = test$Expp)
```

# Interprétation des meilleurs modèles

## GLM

```{r}
coef <- coef(mod.base)
eval_age <- function(age){
    age <- age/75
    100 * exp(sum(age^(1:7) * coef[12:18])) / exp(sum((18/75)^(1:7) * coef[12:18]))
}
xx <- seq(18, 75, 0.001)
yy <- sapply(xx, eval_age)
ggplot() + geom_line(aes(x=xx, y=yy), linewidth=1) + theme_bw() + 
        labs(x="Age", y="% de la prévision à Age=18",
             title = "Effet de la variable Age")
```

```{r}

# Premièrement, nous allons chercher les coefficients du GLM
coef <- summary(mod.base)$coefficients[,1]

diff_dat <- data.frame(differentiel=exp(coef))
variables <- rownames(diff_dat)

# On enlève intercept et les ages
diff_dat <- cbind(variables, diff_dat)[-c(1, 12:21, 27), ]


# On trace le graphique des coefficients des glms
ggplot(diff_dat, aes(y = differentiel, x = variables, fill = differentiel)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Variable", y = "Différentiel", fill= "Différentiel",
       title = "Différentiels des variables catégorielles (GLM)")+
  scale_fill_continuous(low = "gold",
  high = "firebrick") + theme_bw()+
  geom_text(aes(label= round(differentiel, 2)))
```

La variable `Bonus` est la plus importante. Son effet sur la prime est intuitif, puisqu'à chaque fois que la variable augmente de 1, la prime augmente d'environ 1.086. L'étendue de cette variable est [-50, 150]. Ceci donne un diminution de 41.7 % pour les meilleurs personnes assurées et une augmentation de 405 % pour les pires conducteurs le tout par rapport à un assuré qui a un `Bonus` de 0.

Une autre variable importante est la puissance du véhicule. Lorsque le logarithme de la variable augmente de 1, la prévision augmente d'environ 79.9 %. La plus grande augmentation de prévision possible par rapport à la plus petite puissance est de 481 %.

Finalement les deux dernières variables `Density` et `Poldur` ont un signal logique. À chaque fois que la densité de population augmente de 10, la prévision augmente d'environ 4.83 %. À chaque fois que le l’ancienneté du client augmente de 1, la prime diminue d’environ 2.60 %.

## Gradient Boosting

```{r}
predict.fun <- function(model, newdata){
    newData_x <- test
    predict(mod.boost, newdata = newdata,
            type = "response", n.trees=best.iter.cv) * newdata$Expp 
}

boost.iml <- Predictor$new(
  model = mod.boost,
  data = test[, -11],
  y = test$Numtppd,
  predict.fun = predict.fun,
  class = "regression"
  )
```

```{r message=FALSE, warning=FALSE}
boost.imp <- iml::FeatureImp$new(boost.iml, loss = "ce", compare = "difference",
                                 n.repetitions = 1)
```

```{r}
plot(boost.imp)
```

# Conclusion

# Bibliographie
